{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekvRPxwMqbL3"
      },
      "source": [
        "**Artificial Neural Network (ANN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThhUGliUD9il"
      },
      "source": [
        "# x = [2,3]\n",
        "# y = 1\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNcvvwrZFy20"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4knQ10rSF8c1"
      },
      "source": [
        "x = [[2,3]]\n",
        "y = [1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq5LRH6aGHTl"
      },
      "source": [
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(2), activation='logistic', max_iter=1000)\n",
        "mlp.fit(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHS755f8QCJk"
      },
      "source": [
        "# Multilayer Perceptron \n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRoyPgzFGzGn"
      },
      "source": [
        "X = [[1,0,1]]\n",
        "Y = [[0,1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze-yJT9_QZ0g"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(5,2), activation='relu', max_iter=2000, verbose=True)\n",
        "mlp.fit(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLaItZV_cFKz"
      },
      "source": [
        "**Handwritten Digits Recognition with MNIST Dataset using ANN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTUNmOHAQ8Fk"
      },
      "source": [
        "# Load Library\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3d67IZbcz3h"
      },
      "source": [
        "digits = datasets.load_digits()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qs4mvd9c-pX",
        "outputId": "0670e027-efcc-46ff-986a-e0d2e9bd80f4"
      },
      "source": [
        "digits.images.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb8hLO6EdVuy",
        "outputId": "616c910d-5d18-4ea4-f10e-4848f15f9b08"
      },
      "source": [
        "print(digits.images[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
            " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
            " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
            " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
            " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
            " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
            " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
            " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qkfiOoZ2d3-m",
        "outputId": "c9d9c37f-ba67-434f-b1e1-a6c20e54a100"
      },
      "source": [
        "plt.imshow(digits.images[0],cmap='binary')\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JQUunQVQhQyN",
        "outputId": "78de730d-5a51-4db2-af86-17e8d995e96d"
      },
      "source": [
        "plt.imshow(digits.images[1796],cmap='binary')\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67u7b6sEgQwh",
        "outputId": "b3c11394-1f56-4e7b-b392-e0d9db7cd367"
      },
      "source": [
        "print(digits.target)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 ... 8 9 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "DwBUuvgZh1Y-",
        "outputId": "50d4a749-f84e-4c37-ff99-f4f00242aec8"
      },
      "source": [
        "plt.imshow(digits.images[1795],cmap='binary')\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUE21KI_iBGk",
        "outputId": "31d310f5-d7a6-43c8-f103-d77dc90ea3cd"
      },
      "source": [
        "digits.target.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMO01zfFiKwq"
      },
      "source": [
        "y = digits.target\n",
        "x = digits.images"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIat8j67i6sb",
        "outputId": "ec2405ba-47ce-48fc-d036-f793230b5b74"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqkrpK0jjX_P"
      },
      "source": [
        "x = digits.images.reshape(len(digits.images),-1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr50nEVLjdR1",
        "outputId": "93f04970-90c9-4eba-df90-7311f2b3ad46"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bQP1AEbkEAz",
        "outputId": "aa320466-34ed-40fc-c83e-0237f72e85cf"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36xpnl2OkPR-"
      },
      "source": [
        "X_train = x[:1200]\n",
        "X_test = x [1200:]\n",
        "\n",
        "y_train = y[:1200]\n",
        "y_test  = y[1200:]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GY8lhQ5kkj0",
        "outputId": "c6183188-b092-4c62-efda-9c91991140f0"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdcXKqnYk0_J",
        "outputId": "9f777fa0-4b6b-4907-e455-5bafc71e714b"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(597, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYVxSI7vk4Aq",
        "outputId": "581e7657-27be-44cd-b7d9-3fabb00c2e19"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni4BgATFlEdn",
        "outputId": "f365d332-0147-443b-a8aa-08c45ef16c69"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(597,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9enIBi1VlGi2",
        "outputId": "0d755e4d-4404-4319-a4b7-8304b197f711"
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(15), activation='logistic', max_iter=1000, verbose=True)\n",
        "mlp.fit(X_train, y_train)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.39142827\n",
            "Iteration 2, loss = 2.32013339\n",
            "Iteration 3, loss = 2.27109236\n",
            "Iteration 4, loss = 2.23389640\n",
            "Iteration 5, loss = 2.19936056\n",
            "Iteration 6, loss = 2.16064738\n",
            "Iteration 7, loss = 2.11702779\n",
            "Iteration 8, loss = 2.07781518\n",
            "Iteration 9, loss = 2.03944771\n",
            "Iteration 10, loss = 2.00293461\n",
            "Iteration 11, loss = 1.96926754\n",
            "Iteration 12, loss = 1.93660367\n",
            "Iteration 13, loss = 1.90308178\n",
            "Iteration 14, loss = 1.86759079\n",
            "Iteration 15, loss = 1.83460601\n",
            "Iteration 16, loss = 1.80373801\n",
            "Iteration 17, loss = 1.77445627\n",
            "Iteration 18, loss = 1.74617210\n",
            "Iteration 19, loss = 1.71890531\n",
            "Iteration 20, loss = 1.69231372\n",
            "Iteration 21, loss = 1.66620383\n",
            "Iteration 22, loss = 1.64065462\n",
            "Iteration 23, loss = 1.61497826\n",
            "Iteration 24, loss = 1.59070515\n",
            "Iteration 25, loss = 1.56605604\n",
            "Iteration 26, loss = 1.54164621\n",
            "Iteration 27, loss = 1.51857309\n",
            "Iteration 28, loss = 1.49472870\n",
            "Iteration 29, loss = 1.47196525\n",
            "Iteration 30, loss = 1.44878807\n",
            "Iteration 31, loss = 1.42661377\n",
            "Iteration 32, loss = 1.40440986\n",
            "Iteration 33, loss = 1.38268814\n",
            "Iteration 34, loss = 1.36119125\n",
            "Iteration 35, loss = 1.33949412\n",
            "Iteration 36, loss = 1.31855523\n",
            "Iteration 37, loss = 1.29787986\n",
            "Iteration 38, loss = 1.27714810\n",
            "Iteration 39, loss = 1.25745072\n",
            "Iteration 40, loss = 1.23752226\n",
            "Iteration 41, loss = 1.21763286\n",
            "Iteration 42, loss = 1.19858813\n",
            "Iteration 43, loss = 1.17915798\n",
            "Iteration 44, loss = 1.16042585\n",
            "Iteration 45, loss = 1.14180857\n",
            "Iteration 46, loss = 1.12383628\n",
            "Iteration 47, loss = 1.10542248\n",
            "Iteration 48, loss = 1.08775676\n",
            "Iteration 49, loss = 1.06984347\n",
            "Iteration 50, loss = 1.05234046\n",
            "Iteration 51, loss = 1.03577428\n",
            "Iteration 52, loss = 1.01879097\n",
            "Iteration 53, loss = 1.00227292\n",
            "Iteration 54, loss = 0.98580734\n",
            "Iteration 55, loss = 0.97052706\n",
            "Iteration 56, loss = 0.95432863\n",
            "Iteration 57, loss = 0.93955579\n",
            "Iteration 58, loss = 0.92465761\n",
            "Iteration 59, loss = 0.90939977\n",
            "Iteration 60, loss = 0.89518617\n",
            "Iteration 61, loss = 0.88102051\n",
            "Iteration 62, loss = 0.86704361\n",
            "Iteration 63, loss = 0.85326088\n",
            "Iteration 64, loss = 0.84002440\n",
            "Iteration 65, loss = 0.82671640\n",
            "Iteration 66, loss = 0.81397208\n",
            "Iteration 67, loss = 0.80071269\n",
            "Iteration 68, loss = 0.78822014\n",
            "Iteration 69, loss = 0.77529435\n",
            "Iteration 70, loss = 0.76322305\n",
            "Iteration 71, loss = 0.75112347\n",
            "Iteration 72, loss = 0.73916240\n",
            "Iteration 73, loss = 0.72732301\n",
            "Iteration 74, loss = 0.71618652\n",
            "Iteration 75, loss = 0.70457858\n",
            "Iteration 76, loss = 0.69343308\n",
            "Iteration 77, loss = 0.68265621\n",
            "Iteration 78, loss = 0.67170329\n",
            "Iteration 79, loss = 0.66145658\n",
            "Iteration 80, loss = 0.65104107\n",
            "Iteration 81, loss = 0.64084967\n",
            "Iteration 82, loss = 0.63032450\n",
            "Iteration 83, loss = 0.61842609\n",
            "Iteration 84, loss = 0.60473564\n",
            "Iteration 85, loss = 0.59168840\n",
            "Iteration 86, loss = 0.57859987\n",
            "Iteration 87, loss = 0.56553486\n",
            "Iteration 88, loss = 0.55365653\n",
            "Iteration 89, loss = 0.54214920\n",
            "Iteration 90, loss = 0.53138609\n",
            "Iteration 91, loss = 0.52058073\n",
            "Iteration 92, loss = 0.51128246\n",
            "Iteration 93, loss = 0.50155570\n",
            "Iteration 94, loss = 0.49271286\n",
            "Iteration 95, loss = 0.48386945\n",
            "Iteration 96, loss = 0.47601423\n",
            "Iteration 97, loss = 0.46769914\n",
            "Iteration 98, loss = 0.46043329\n",
            "Iteration 99, loss = 0.45293683\n",
            "Iteration 100, loss = 0.44554665\n",
            "Iteration 101, loss = 0.43861239\n",
            "Iteration 102, loss = 0.43146517\n",
            "Iteration 103, loss = 0.42490265\n",
            "Iteration 104, loss = 0.41814799\n",
            "Iteration 105, loss = 0.41162714\n",
            "Iteration 106, loss = 0.40536924\n",
            "Iteration 107, loss = 0.39976701\n",
            "Iteration 108, loss = 0.39371010\n",
            "Iteration 109, loss = 0.38782816\n",
            "Iteration 110, loss = 0.38243617\n",
            "Iteration 111, loss = 0.37659788\n",
            "Iteration 112, loss = 0.37146772\n",
            "Iteration 113, loss = 0.36635851\n",
            "Iteration 114, loss = 0.36125769\n",
            "Iteration 115, loss = 0.35645859\n",
            "Iteration 116, loss = 0.35147502\n",
            "Iteration 117, loss = 0.34679473\n",
            "Iteration 118, loss = 0.34194069\n",
            "Iteration 119, loss = 0.33735744\n",
            "Iteration 120, loss = 0.33362123\n",
            "Iteration 121, loss = 0.32886078\n",
            "Iteration 122, loss = 0.32479682\n",
            "Iteration 123, loss = 0.32095443\n",
            "Iteration 124, loss = 0.31651591\n",
            "Iteration 125, loss = 0.31271061\n",
            "Iteration 126, loss = 0.30846125\n",
            "Iteration 127, loss = 0.30477420\n",
            "Iteration 128, loss = 0.30094060\n",
            "Iteration 129, loss = 0.29744899\n",
            "Iteration 130, loss = 0.29405143\n",
            "Iteration 131, loss = 0.29064242\n",
            "Iteration 132, loss = 0.28723654\n",
            "Iteration 133, loss = 0.28386883\n",
            "Iteration 134, loss = 0.28048017\n",
            "Iteration 135, loss = 0.27713587\n",
            "Iteration 136, loss = 0.27391994\n",
            "Iteration 137, loss = 0.27056542\n",
            "Iteration 138, loss = 0.26766202\n",
            "Iteration 139, loss = 0.26469279\n",
            "Iteration 140, loss = 0.26181126\n",
            "Iteration 141, loss = 0.25881063\n",
            "Iteration 142, loss = 0.25612438\n",
            "Iteration 143, loss = 0.25336734\n",
            "Iteration 144, loss = 0.25057002\n",
            "Iteration 145, loss = 0.24781079\n",
            "Iteration 146, loss = 0.24535896\n",
            "Iteration 147, loss = 0.24279769\n",
            "Iteration 148, loss = 0.24016110\n",
            "Iteration 149, loss = 0.23766682\n",
            "Iteration 150, loss = 0.23540619\n",
            "Iteration 151, loss = 0.23294287\n",
            "Iteration 152, loss = 0.23064293\n",
            "Iteration 153, loss = 0.22817224\n",
            "Iteration 154, loss = 0.22599193\n",
            "Iteration 155, loss = 0.22368341\n",
            "Iteration 156, loss = 0.22144995\n",
            "Iteration 157, loss = 0.21934587\n",
            "Iteration 158, loss = 0.21724837\n",
            "Iteration 159, loss = 0.21502076\n",
            "Iteration 160, loss = 0.21304191\n",
            "Iteration 161, loss = 0.21097138\n",
            "Iteration 162, loss = 0.20902924\n",
            "Iteration 163, loss = 0.20691219\n",
            "Iteration 164, loss = 0.20510063\n",
            "Iteration 165, loss = 0.20304181\n",
            "Iteration 166, loss = 0.20148911\n",
            "Iteration 167, loss = 0.19951309\n",
            "Iteration 168, loss = 0.19767445\n",
            "Iteration 169, loss = 0.19583442\n",
            "Iteration 170, loss = 0.19396635\n",
            "Iteration 171, loss = 0.19219492\n",
            "Iteration 172, loss = 0.19040485\n",
            "Iteration 173, loss = 0.18929408\n",
            "Iteration 174, loss = 0.18708562\n",
            "Iteration 175, loss = 0.18546724\n",
            "Iteration 176, loss = 0.18391115\n",
            "Iteration 177, loss = 0.18240703\n",
            "Iteration 178, loss = 0.18055868\n",
            "Iteration 179, loss = 0.17917327\n",
            "Iteration 180, loss = 0.17764103\n",
            "Iteration 181, loss = 0.17605820\n",
            "Iteration 182, loss = 0.17448889\n",
            "Iteration 183, loss = 0.17301865\n",
            "Iteration 184, loss = 0.17161772\n",
            "Iteration 185, loss = 0.17010613\n",
            "Iteration 186, loss = 0.16876188\n",
            "Iteration 187, loss = 0.16726866\n",
            "Iteration 188, loss = 0.16601309\n",
            "Iteration 189, loss = 0.16474655\n",
            "Iteration 190, loss = 0.16317547\n",
            "Iteration 191, loss = 0.16222368\n",
            "Iteration 192, loss = 0.16072223\n",
            "Iteration 193, loss = 0.15942124\n",
            "Iteration 194, loss = 0.15803716\n",
            "Iteration 195, loss = 0.15671139\n",
            "Iteration 196, loss = 0.15575251\n",
            "Iteration 197, loss = 0.15428266\n",
            "Iteration 198, loss = 0.15325505\n",
            "Iteration 199, loss = 0.15180266\n",
            "Iteration 200, loss = 0.15086033\n",
            "Iteration 201, loss = 0.14949538\n",
            "Iteration 202, loss = 0.14853928\n",
            "Iteration 203, loss = 0.14723672\n",
            "Iteration 204, loss = 0.14618839\n",
            "Iteration 205, loss = 0.14496773\n",
            "Iteration 206, loss = 0.14388279\n",
            "Iteration 207, loss = 0.14290090\n",
            "Iteration 208, loss = 0.14170260\n",
            "Iteration 209, loss = 0.14084776\n",
            "Iteration 210, loss = 0.13954230\n",
            "Iteration 211, loss = 0.13856161\n",
            "Iteration 212, loss = 0.13756247\n",
            "Iteration 213, loss = 0.13657190\n",
            "Iteration 214, loss = 0.13556437\n",
            "Iteration 215, loss = 0.13455185\n",
            "Iteration 216, loss = 0.13371143\n",
            "Iteration 217, loss = 0.13253855\n",
            "Iteration 218, loss = 0.13185491\n",
            "Iteration 219, loss = 0.13085178\n",
            "Iteration 220, loss = 0.12988445\n",
            "Iteration 221, loss = 0.12883498\n",
            "Iteration 222, loss = 0.12787670\n",
            "Iteration 223, loss = 0.12706667\n",
            "Iteration 224, loss = 0.12614688\n",
            "Iteration 225, loss = 0.12513229\n",
            "Iteration 226, loss = 0.12439091\n",
            "Iteration 227, loss = 0.12349997\n",
            "Iteration 228, loss = 0.12270856\n",
            "Iteration 229, loss = 0.12171437\n",
            "Iteration 230, loss = 0.12095030\n",
            "Iteration 231, loss = 0.12023077\n",
            "Iteration 232, loss = 0.11921502\n",
            "Iteration 233, loss = 0.11844064\n",
            "Iteration 234, loss = 0.11775316\n",
            "Iteration 235, loss = 0.11690840\n",
            "Iteration 236, loss = 0.11600688\n",
            "Iteration 237, loss = 0.11538960\n",
            "Iteration 238, loss = 0.11437505\n",
            "Iteration 239, loss = 0.11363050\n",
            "Iteration 240, loss = 0.11284900\n",
            "Iteration 241, loss = 0.11231146\n",
            "Iteration 242, loss = 0.11130223\n",
            "Iteration 243, loss = 0.11069271\n",
            "Iteration 244, loss = 0.10991682\n",
            "Iteration 245, loss = 0.10914037\n",
            "Iteration 246, loss = 0.10851521\n",
            "Iteration 247, loss = 0.10783093\n",
            "Iteration 248, loss = 0.10689458\n",
            "Iteration 249, loss = 0.10642458\n",
            "Iteration 250, loss = 0.10575397\n",
            "Iteration 251, loss = 0.10498313\n",
            "Iteration 252, loss = 0.10421442\n",
            "Iteration 253, loss = 0.10347552\n",
            "Iteration 254, loss = 0.10293678\n",
            "Iteration 255, loss = 0.10217964\n",
            "Iteration 256, loss = 0.10139421\n",
            "Iteration 257, loss = 0.10084222\n",
            "Iteration 258, loss = 0.10011689\n",
            "Iteration 259, loss = 0.09955325\n",
            "Iteration 260, loss = 0.09885580\n",
            "Iteration 261, loss = 0.09833614\n",
            "Iteration 262, loss = 0.09768566\n",
            "Iteration 263, loss = 0.09694824\n",
            "Iteration 264, loss = 0.09637770\n",
            "Iteration 265, loss = 0.09580498\n",
            "Iteration 266, loss = 0.09512478\n",
            "Iteration 267, loss = 0.09446906\n",
            "Iteration 268, loss = 0.09417752\n",
            "Iteration 269, loss = 0.09336596\n",
            "Iteration 270, loss = 0.09278722\n",
            "Iteration 271, loss = 0.09217820\n",
            "Iteration 272, loss = 0.09186371\n",
            "Iteration 273, loss = 0.09119682\n",
            "Iteration 274, loss = 0.09060452\n",
            "Iteration 275, loss = 0.08995909\n",
            "Iteration 276, loss = 0.08943696\n",
            "Iteration 277, loss = 0.08886532\n",
            "Iteration 278, loss = 0.08824048\n",
            "Iteration 279, loss = 0.08772756\n",
            "Iteration 280, loss = 0.08720265\n",
            "Iteration 281, loss = 0.08663486\n",
            "Iteration 282, loss = 0.08616391\n",
            "Iteration 283, loss = 0.08565233\n",
            "Iteration 284, loss = 0.08521810\n",
            "Iteration 285, loss = 0.08475551\n",
            "Iteration 286, loss = 0.08412242\n",
            "Iteration 287, loss = 0.08375701\n",
            "Iteration 288, loss = 0.08310229\n",
            "Iteration 289, loss = 0.08258968\n",
            "Iteration 290, loss = 0.08217071\n",
            "Iteration 291, loss = 0.08188979\n",
            "Iteration 292, loss = 0.08122584\n",
            "Iteration 293, loss = 0.08075671\n",
            "Iteration 294, loss = 0.08023823\n",
            "Iteration 295, loss = 0.07973683\n",
            "Iteration 296, loss = 0.07928481\n",
            "Iteration 297, loss = 0.07884077\n",
            "Iteration 298, loss = 0.07835929\n",
            "Iteration 299, loss = 0.07799287\n",
            "Iteration 300, loss = 0.07744497\n",
            "Iteration 301, loss = 0.07701984\n",
            "Iteration 302, loss = 0.07650294\n",
            "Iteration 303, loss = 0.07625219\n",
            "Iteration 304, loss = 0.07574875\n",
            "Iteration 305, loss = 0.07532558\n",
            "Iteration 306, loss = 0.07483693\n",
            "Iteration 307, loss = 0.07452844\n",
            "Iteration 308, loss = 0.07411411\n",
            "Iteration 309, loss = 0.07363554\n",
            "Iteration 310, loss = 0.07325505\n",
            "Iteration 311, loss = 0.07273484\n",
            "Iteration 312, loss = 0.07245536\n",
            "Iteration 313, loss = 0.07191471\n",
            "Iteration 314, loss = 0.07159363\n",
            "Iteration 315, loss = 0.07113925\n",
            "Iteration 316, loss = 0.07086158\n",
            "Iteration 317, loss = 0.07034835\n",
            "Iteration 318, loss = 0.07001883\n",
            "Iteration 319, loss = 0.06976202\n",
            "Iteration 320, loss = 0.06935892\n",
            "Iteration 321, loss = 0.06898076\n",
            "Iteration 322, loss = 0.06848623\n",
            "Iteration 323, loss = 0.06813830\n",
            "Iteration 324, loss = 0.06799024\n",
            "Iteration 325, loss = 0.06744664\n",
            "Iteration 326, loss = 0.06711930\n",
            "Iteration 327, loss = 0.06674280\n",
            "Iteration 328, loss = 0.06637248\n",
            "Iteration 329, loss = 0.06598745\n",
            "Iteration 330, loss = 0.06576664\n",
            "Iteration 331, loss = 0.06533317\n",
            "Iteration 332, loss = 0.06499459\n",
            "Iteration 333, loss = 0.06460384\n",
            "Iteration 334, loss = 0.06436221\n",
            "Iteration 335, loss = 0.06399808\n",
            "Iteration 336, loss = 0.06371779\n",
            "Iteration 337, loss = 0.06337826\n",
            "Iteration 338, loss = 0.06295703\n",
            "Iteration 339, loss = 0.06262469\n",
            "Iteration 340, loss = 0.06229998\n",
            "Iteration 341, loss = 0.06198842\n",
            "Iteration 342, loss = 0.06170155\n",
            "Iteration 343, loss = 0.06138193\n",
            "Iteration 344, loss = 0.06110605\n",
            "Iteration 345, loss = 0.06074451\n",
            "Iteration 346, loss = 0.06045240\n",
            "Iteration 347, loss = 0.06012172\n",
            "Iteration 348, loss = 0.05982675\n",
            "Iteration 349, loss = 0.05952240\n",
            "Iteration 350, loss = 0.05928662\n",
            "Iteration 351, loss = 0.05884768\n",
            "Iteration 352, loss = 0.05855208\n",
            "Iteration 353, loss = 0.05826173\n",
            "Iteration 354, loss = 0.05806159\n",
            "Iteration 355, loss = 0.05770502\n",
            "Iteration 356, loss = 0.05758407\n",
            "Iteration 357, loss = 0.05706964\n",
            "Iteration 358, loss = 0.05685705\n",
            "Iteration 359, loss = 0.05656466\n",
            "Iteration 360, loss = 0.05627153\n",
            "Iteration 361, loss = 0.05605409\n",
            "Iteration 362, loss = 0.05572769\n",
            "Iteration 363, loss = 0.05546513\n",
            "Iteration 364, loss = 0.05522878\n",
            "Iteration 365, loss = 0.05498738\n",
            "Iteration 366, loss = 0.05472466\n",
            "Iteration 367, loss = 0.05441148\n",
            "Iteration 368, loss = 0.05407659\n",
            "Iteration 369, loss = 0.05382324\n",
            "Iteration 370, loss = 0.05350458\n",
            "Iteration 371, loss = 0.05341372\n",
            "Iteration 372, loss = 0.05308229\n",
            "Iteration 373, loss = 0.05273833\n",
            "Iteration 374, loss = 0.05252664\n",
            "Iteration 375, loss = 0.05224434\n",
            "Iteration 376, loss = 0.05198868\n",
            "Iteration 377, loss = 0.05178799\n",
            "Iteration 378, loss = 0.05149666\n",
            "Iteration 379, loss = 0.05125880\n",
            "Iteration 380, loss = 0.05101141\n",
            "Iteration 381, loss = 0.05075660\n",
            "Iteration 382, loss = 0.05052185\n",
            "Iteration 383, loss = 0.05034260\n",
            "Iteration 384, loss = 0.05007932\n",
            "Iteration 385, loss = 0.04984410\n",
            "Iteration 386, loss = 0.04956792\n",
            "Iteration 387, loss = 0.04936143\n",
            "Iteration 388, loss = 0.04916861\n",
            "Iteration 389, loss = 0.04890313\n",
            "Iteration 390, loss = 0.04872557\n",
            "Iteration 391, loss = 0.04843115\n",
            "Iteration 392, loss = 0.04827515\n",
            "Iteration 393, loss = 0.04802678\n",
            "Iteration 394, loss = 0.04779200\n",
            "Iteration 395, loss = 0.04751798\n",
            "Iteration 396, loss = 0.04730266\n",
            "Iteration 397, loss = 0.04706943\n",
            "Iteration 398, loss = 0.04695421\n",
            "Iteration 399, loss = 0.04672515\n",
            "Iteration 400, loss = 0.04640646\n",
            "Iteration 401, loss = 0.04622485\n",
            "Iteration 402, loss = 0.04603201\n",
            "Iteration 403, loss = 0.04581471\n",
            "Iteration 404, loss = 0.04562366\n",
            "Iteration 405, loss = 0.04536097\n",
            "Iteration 406, loss = 0.04518867\n",
            "Iteration 407, loss = 0.04498645\n",
            "Iteration 408, loss = 0.04472952\n",
            "Iteration 409, loss = 0.04452613\n",
            "Iteration 410, loss = 0.04440072\n",
            "Iteration 411, loss = 0.04420420\n",
            "Iteration 412, loss = 0.04396263\n",
            "Iteration 413, loss = 0.04380152\n",
            "Iteration 414, loss = 0.04353152\n",
            "Iteration 415, loss = 0.04339495\n",
            "Iteration 416, loss = 0.04315032\n",
            "Iteration 417, loss = 0.04296729\n",
            "Iteration 418, loss = 0.04279169\n",
            "Iteration 419, loss = 0.04258120\n",
            "Iteration 420, loss = 0.04235307\n",
            "Iteration 421, loss = 0.04225180\n",
            "Iteration 422, loss = 0.04203656\n",
            "Iteration 423, loss = 0.04178474\n",
            "Iteration 424, loss = 0.04169073\n",
            "Iteration 425, loss = 0.04142315\n",
            "Iteration 426, loss = 0.04125488\n",
            "Iteration 427, loss = 0.04103625\n",
            "Iteration 428, loss = 0.04094554\n",
            "Iteration 429, loss = 0.04068464\n",
            "Iteration 430, loss = 0.04049127\n",
            "Iteration 431, loss = 0.04032018\n",
            "Iteration 432, loss = 0.04010848\n",
            "Iteration 433, loss = 0.04002172\n",
            "Iteration 434, loss = 0.03975833\n",
            "Iteration 435, loss = 0.03961892\n",
            "Iteration 436, loss = 0.03946757\n",
            "Iteration 437, loss = 0.03929482\n",
            "Iteration 438, loss = 0.03911532\n",
            "Iteration 439, loss = 0.03893831\n",
            "Iteration 440, loss = 0.03876469\n",
            "Iteration 441, loss = 0.03858320\n",
            "Iteration 442, loss = 0.03840476\n",
            "Iteration 443, loss = 0.03826170\n",
            "Iteration 444, loss = 0.03810827\n",
            "Iteration 445, loss = 0.03795223\n",
            "Iteration 446, loss = 0.03770945\n",
            "Iteration 447, loss = 0.03756276\n",
            "Iteration 448, loss = 0.03749114\n",
            "Iteration 449, loss = 0.03727830\n",
            "Iteration 450, loss = 0.03716995\n",
            "Iteration 451, loss = 0.03693600\n",
            "Iteration 452, loss = 0.03680460\n",
            "Iteration 453, loss = 0.03660457\n",
            "Iteration 454, loss = 0.03646048\n",
            "Iteration 455, loss = 0.03631822\n",
            "Iteration 456, loss = 0.03614880\n",
            "Iteration 457, loss = 0.03596764\n",
            "Iteration 458, loss = 0.03581281\n",
            "Iteration 459, loss = 0.03569509\n",
            "Iteration 460, loss = 0.03556651\n",
            "Iteration 461, loss = 0.03536495\n",
            "Iteration 462, loss = 0.03528307\n",
            "Iteration 463, loss = 0.03507464\n",
            "Iteration 464, loss = 0.03497409\n",
            "Iteration 465, loss = 0.03481925\n",
            "Iteration 466, loss = 0.03466944\n",
            "Iteration 467, loss = 0.03450020\n",
            "Iteration 468, loss = 0.03436479\n",
            "Iteration 469, loss = 0.03418989\n",
            "Iteration 470, loss = 0.03406073\n",
            "Iteration 471, loss = 0.03390612\n",
            "Iteration 472, loss = 0.03378726\n",
            "Iteration 473, loss = 0.03364259\n",
            "Iteration 474, loss = 0.03348670\n",
            "Iteration 475, loss = 0.03334792\n",
            "Iteration 476, loss = 0.03321767\n",
            "Iteration 477, loss = 0.03309406\n",
            "Iteration 478, loss = 0.03290484\n",
            "Iteration 479, loss = 0.03287967\n",
            "Iteration 480, loss = 0.03270083\n",
            "Iteration 481, loss = 0.03261039\n",
            "Iteration 482, loss = 0.03243969\n",
            "Iteration 483, loss = 0.03224664\n",
            "Iteration 484, loss = 0.03212161\n",
            "Iteration 485, loss = 0.03199490\n",
            "Iteration 486, loss = 0.03183954\n",
            "Iteration 487, loss = 0.03169556\n",
            "Iteration 488, loss = 0.03157764\n",
            "Iteration 489, loss = 0.03147366\n",
            "Iteration 490, loss = 0.03135532\n",
            "Iteration 491, loss = 0.03126998\n",
            "Iteration 492, loss = 0.03110573\n",
            "Iteration 493, loss = 0.03095729\n",
            "Iteration 494, loss = 0.03082382\n",
            "Iteration 495, loss = 0.03071173\n",
            "Iteration 496, loss = 0.03058299\n",
            "Iteration 497, loss = 0.03044622\n",
            "Iteration 498, loss = 0.03037543\n",
            "Iteration 499, loss = 0.03021177\n",
            "Iteration 500, loss = 0.03009976\n",
            "Iteration 501, loss = 0.02998186\n",
            "Iteration 502, loss = 0.02989888\n",
            "Iteration 503, loss = 0.02975872\n",
            "Iteration 504, loss = 0.02964774\n",
            "Iteration 505, loss = 0.02952577\n",
            "Iteration 506, loss = 0.02938724\n",
            "Iteration 507, loss = 0.02925799\n",
            "Iteration 508, loss = 0.02915708\n",
            "Iteration 509, loss = 0.02905526\n",
            "Iteration 510, loss = 0.02890971\n",
            "Iteration 511, loss = 0.02877757\n",
            "Iteration 512, loss = 0.02866988\n",
            "Iteration 513, loss = 0.02860402\n",
            "Iteration 514, loss = 0.02843765\n",
            "Iteration 515, loss = 0.02838659\n",
            "Iteration 516, loss = 0.02822725\n",
            "Iteration 517, loss = 0.02813749\n",
            "Iteration 518, loss = 0.02803938\n",
            "Iteration 519, loss = 0.02791038\n",
            "Iteration 520, loss = 0.02780079\n",
            "Iteration 521, loss = 0.02769465\n",
            "Iteration 522, loss = 0.02760400\n",
            "Iteration 523, loss = 0.02749757\n",
            "Iteration 524, loss = 0.02738769\n",
            "Iteration 525, loss = 0.02725819\n",
            "Iteration 526, loss = 0.02713210\n",
            "Iteration 527, loss = 0.02708778\n",
            "Iteration 528, loss = 0.02693043\n",
            "Iteration 529, loss = 0.02689299\n",
            "Iteration 530, loss = 0.02679510\n",
            "Iteration 531, loss = 0.02663053\n",
            "Iteration 532, loss = 0.02657320\n",
            "Iteration 533, loss = 0.02644066\n",
            "Iteration 534, loss = 0.02635476\n",
            "Iteration 535, loss = 0.02625312\n",
            "Iteration 536, loss = 0.02612053\n",
            "Iteration 537, loss = 0.02602898\n",
            "Iteration 538, loss = 0.02597854\n",
            "Iteration 539, loss = 0.02584413\n",
            "Iteration 540, loss = 0.02576275\n",
            "Iteration 541, loss = 0.02564272\n",
            "Iteration 542, loss = 0.02556750\n",
            "Iteration 543, loss = 0.02543021\n",
            "Iteration 544, loss = 0.02538635\n",
            "Iteration 545, loss = 0.02525550\n",
            "Iteration 546, loss = 0.02517561\n",
            "Iteration 547, loss = 0.02509562\n",
            "Iteration 548, loss = 0.02497783\n",
            "Iteration 549, loss = 0.02489741\n",
            "Iteration 550, loss = 0.02481489\n",
            "Iteration 551, loss = 0.02469248\n",
            "Iteration 552, loss = 0.02459132\n",
            "Iteration 553, loss = 0.02452768\n",
            "Iteration 554, loss = 0.02441205\n",
            "Iteration 555, loss = 0.02433835\n",
            "Iteration 556, loss = 0.02423383\n",
            "Iteration 557, loss = 0.02415569\n",
            "Iteration 558, loss = 0.02405612\n",
            "Iteration 559, loss = 0.02398498\n",
            "Iteration 560, loss = 0.02390683\n",
            "Iteration 561, loss = 0.02380847\n",
            "Iteration 562, loss = 0.02373286\n",
            "Iteration 563, loss = 0.02364002\n",
            "Iteration 564, loss = 0.02359439\n",
            "Iteration 565, loss = 0.02346417\n",
            "Iteration 566, loss = 0.02339181\n",
            "Iteration 567, loss = 0.02328848\n",
            "Iteration 568, loss = 0.02322399\n",
            "Iteration 569, loss = 0.02313166\n",
            "Iteration 570, loss = 0.02308042\n",
            "Iteration 571, loss = 0.02295145\n",
            "Iteration 572, loss = 0.02289330\n",
            "Iteration 573, loss = 0.02282078\n",
            "Iteration 574, loss = 0.02270949\n",
            "Iteration 575, loss = 0.02261233\n",
            "Iteration 576, loss = 0.02251295\n",
            "Iteration 577, loss = 0.02247002\n",
            "Iteration 578, loss = 0.02238106\n",
            "Iteration 579, loss = 0.02229406\n",
            "Iteration 580, loss = 0.02221360\n",
            "Iteration 581, loss = 0.02214556\n",
            "Iteration 582, loss = 0.02205193\n",
            "Iteration 583, loss = 0.02197840\n",
            "Iteration 584, loss = 0.02188905\n",
            "Iteration 585, loss = 0.02184354\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
              "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=15, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffb3P4QIlf17"
      },
      "source": [
        "test_image = X_test[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfxmR8jgmIxB",
        "outputId": "367424da-dc8f-400a-ad09-f24aaec7c517"
      },
      "source": [
        "mlp.predict([test_image])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rwM1e-EmaHh",
        "outputId": "5b6049bf-b422-49e8-a8bc-84377f47078e"
      },
      "source": [
        "y_test[0]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIF5qLkanf4W"
      },
      "source": [
        "prediction = mlp.predict(X_test)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEPhsCvNnB3I",
        "outputId": "195da665-0a36-40b9-e9a9-81f6b2c5eca6"
      },
      "source": [
        "y_test[:50]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4,\n",
              "       6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7,\n",
              "       4, 6, 3, 1, 3, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh2457w7nNjD",
        "outputId": "e04ab7da-4ffd-4da8-b436-02cf3cabf37a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,prediction)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.916247906197655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syOBQJS6n1xi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}